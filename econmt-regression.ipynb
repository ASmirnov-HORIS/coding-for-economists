{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this chapter, you'll learn about to do regressions with code.\n",
    "\n",
    "If you're running this code (either by copying and pasting it, or by downloading it using the icons at the top of the page), you may need to the packages it uses by, for example, running `pip install packagename` on your computer's command line. (If you're not sure what a command line is, take a quick look at the basics of coding chapter.)\n",
    "\n",
    "Most of this chapter will rely on [statsmodels](https://www.statsmodels.org/stable/index.html).\n",
    "\n",
    "Some of the material in this chapter follows [Grant McDermott](https://grantmcdermott.com/)'s excellent notes and the [Library of Statistical Translation](https://lost-stats.github.io/).\n",
    "\n",
    "### Notation and basic definitions\n",
    "\n",
    "Greek letters, like $\\beta$, are the truth and represent parameters. Modified Greek letters are an estimate of the truth, for example $\\hat{\\beta}$. Sometimes Greek letters will stand in for vectors of parameters. Most of the time, upper case Latin characters such as $X$ will represent random variables (which could have more than one dimension). Lower case letters from the Latin alphabet denote realised data, for instance $x$ (which again could be multi-dimensional).  Modified Latin alphabet letters denote computations performed on data, for instance $\\bar{x} = \\frac{1}{n} \\displaystyle\\sum_{i} x_i$ where $n$ is number of samples.\n",
    "\n",
    "Ordinary least squares (OLS) regression is used to predict the value of an outcome variable $y$ based on one or more input predictor variables arrange in a matrix $X$. The equation that we would like to recover is\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2\n",
    "$$\n",
    "\n",
    "where the $x_i$ could be transforms of original data. But this is a platonic ideal, what is called the data generating process (DGP). All we can do is recover *estimates* of it, i.e. we can find $\\hat{\\beta_i}$ and the relationship\n",
    "\n",
    "$$\n",
    "y = \\hat{\\beta_0} + \\hat{\\beta_1} \\cdot x_1 + \\hat{\\beta_2} \\cdot x_2 + \\epsilon\n",
    "$$\n",
    "\n",
    "This equation can also be expressed in matrix form as\n",
    "\n",
    "$$\n",
    "y = x'\\cdot \\hat{\\beta} + \\epsilon\n",
    "$$\n",
    "\n",
    "where $x' = (1, x_1, \\dots, x_{n})'$ and $\\hat{\\beta} = (\\hat{\\beta_0}, \\hat{\\beta_1}, \\dots, \\hat{\\beta_{n}})$.\n",
    "\n",
    "Given data $y_i$ stacked to make a vector $y$ and $x_{i}$ stacked to make a matrix $X$, this can be solved for the coefficients $\\hat{\\beta}$ according to\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\left(X'X\\right)^{-1} X'y\n",
    "$$\n",
    "\n",
    "The interpretation of these regression coefficients depends on what their units are to begin with, but you can always work it out by differentiating both sides of the model equation with respect to the $x_i$. For example, for the first model equation above\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_i} = \\beta_i\n",
    "$$\n",
    "\n",
    "so we get the interpretation that $\\beta_i$ is the rate of change of y with respect to $x_i$. If $x_i$ and $y$ are in levels, this means that a unit increase in $x_i$ is associated with a $\\beta_i$ units increase in $y$. If the right-hand side of the model is $\\ln x_i$ then we get\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_i} = \\beta_i \\frac{1}{x_i} \n",
    "$$\n",
    "\n",
    "with some abuse of notation, we can rewrite this as $dy = \\beta_i dx_i/x_i$, which says that a percent change in $x_i$ is associated with a $\\beta_i$ unit change in $y$. With a logged $y$ variable, it's a percent change in $x_i$ that is associated with a percent change in $y$, or $dy/y = \\beta_i dx_i/x_i$ (note that both sides of this equation are unitless in this case). Finally, another example that is important in practice is that of log differences, eg $y = \\beta_i (\\ln x_i - \\ln x_i')$. Again, we will abuse notation and say that this case may be represented as $dy = \\beta_i (d x_i/x_i - dx_i'/x_i')$, i.e. the difference in two percentages, a *percentage point* change, in $x_i$ is associated with a $\\beta_i$ unit change in $y$.\n",
    "\n",
    "### Imports\n",
    "\n",
    "Let's import the packages we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO hide cell\n",
    "# Set max rows displayed for readability\n",
    "pd.set_option('display.max_rows', 6)\n",
    "# Plot settings\n",
    "plot_style = {'xtick.labelsize': 20,\n",
    "                  'ytick.labelsize': 20,\n",
    "                  'font.size': 22,\n",
    "                  'figure.autolayout': True,\n",
    "                  'figure.figsize': (10, 5.5),\n",
    "                  'axes.titlesize': 22,\n",
    "                  'axes.labelsize': 20,\n",
    "                  'lines.linewidth': 4,\n",
    "                  'lines.markersize': 6,\n",
    "                  'legend.fontsize': 16,\n",
    "                  'mathtext.fontset': 'stix',\n",
    "                  'font.family': 'STIXGeneral',\n",
    "                  'legend.frameon': False}\n",
    "plt.style.use(plot_style)"
   ]
  },
  {
   "source": [
    "## Regression basics\n",
    "\n",
    "There are two ways to run regressions in [**statsmodels**](https://www.statsmodels.org/stable/index.html); passing the data directly as objects, and using formulae. We'll see both but, just to get things started, let's use the formula API.\n",
    "\n",
    "We'll use the starwars dataset to run a regression of mass on height for star wars characters. First, let's bring the dataset in:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join('data', 'starwars.pickle'))\n",
    "# Look at first few rows\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "Okay, now let's do a regression using OLS and a formula that says our y-variable is mass and our regressor is height:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('mass ~ height', data=df).fit()"
   ]
  },
  {
   "source": [
    "Well, where are the results!? They're stored in the object we created. To peek at them we need to call the summary function (and, for easy reading, I'll print it out too using `print`)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "source": [
    "What we're seeing here are really several tables glued together. To just grab the coefficients in a tidy format, use"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary().tables[1]"
   ]
  },
  {
   "source": [
    "You'll have noticed that we got an intercept, even though we didn't specify one in the formula. **statsmodels** adds in an intercept by default because, most of the time, you will want one. To turn it off, add a `-1` at the end of the formula command, eg in this case you would call `smf.ols('mass ~ height -1', data=df).fit()`.\n",
    "\n",
    "The fit we got in the case with the intercept was pretty terrible; a low $R^2$ and both of our confidence intervals are large and contain zero. What's going on? If there's one adage in regression that's always worth paying attention to, it's *always plot your data*. Let's see what's going on here:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=df, x=\"height\", y=\"mass\",\n",
    "                s=250, ax=ax, legend=False,\n",
    "                alpha=0.6)\n",
    "ax.annotate('Jabba the Hutt', df.iloc[df['mass'].idxmax()][['height', 'mass']],\n",
    "            xytext=(0, -50), textcoords='offset points',\n",
    "            arrowprops=dict(arrowstyle=\"fancy\",\n",
    "                            color='k',\n",
    "                            connectionstyle=\"arc3,rad=0.3\",\n",
    "                            )\n",
    "            )\n",
    "ax.set_title('Always look for outliers!', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Oh dear, Jabba's been on the paddy frogs again, and he's a bit of different case. When we're estimating statistical relationships, we have all kinds of choices and should be wary about arbitrary decisions of what to include or exclude in case we fool ourselves about the generality of the relationship we are capturing. Let's say we knew that we weren't interested in Hutts though, but only in other species: in that case, it's fair enough to filter out Jabba and run the regression without this obvious outlier. We'll exclude any entry that contains the string 'Jabba' in the `name` column:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_outlier_free = smf.ols('mass ~ height', data=df[~df['name'].str.contains('Jabba')]).fit()\n",
    "print(results_outlier_free.summary())"
   ]
  },
  {
   "source": [
    "This looks a lot more healthy. Not only is the model explaining a *lot* more of the data, but the coefficients are now significant."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Standard errors\n",
    "\n",
    "You'll have seen that there's a column for the standard error of the estimates in the regression table and a message saying that the covariance type of these is 'nonrobust'. Let's say that, instead, we want to use Eicker-Huber-White robust standard errors, aka \"HC2\" standard errors. We can specify to use these up front standard errors up front in the fit method:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(smf.ols('mass ~ height', data=df)\n",
    "    .fit(cov_type='HC2')\n",
    "    .summary()\n",
    "    .tables[1])"
   ]
  },
  {
   "source": [
    "Or, alternatively, we can go back to our existing results and recompute the results from those:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.get_robustcov_results('HC2').summary())"
   ]
  },
  {
   "source": [
    "There are several different types of standard errors available in **statsmodels**:\n",
    "\n",
    "- ‘HC0’, ‘HC1’, ‘HC2’, and ‘HC3’\n",
    "- ‘HAC’, for heteroskedasticity and autocorrelation consistent standard errors, for which you may want to also use some keyword arguments\n",
    "- 'hac-groupsum’, for Driscoll and Kraay heteroscedasticity and\n",
    "autocorrelation robust standard errors in panel data, again for which you may have to specify extra keyword arguments\n",
    "- 'hac-panel’, for heteroscedasticity and autocorrelation robust standard\n",
    "errors in panel data, again with keyword arguments; and\n",
    "- 'cluster' for clustered standard errors.\n",
    "\n",
    "You can find information on all of these [here](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLSResults.get_robustcov_results.html?highlight=get_robustcov_results#statsmodels.regression.linear_model.OLSResults.get_robustcov_results). For now, let's look more closely at those last ones: clustered standard errors.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Clustered standard errors\n",
    "\n",
    "Often, we know something about the structure of likely errors, namely that they occur in groups. In the below example we use one-way clusters to capture this effect in the errors.\n",
    "\n",
    "Note that in the below example, we grab a subset of the data for which a set of variables we're interested in are defined, otherwise the below example would execute with an error because of missing cluster-group values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf = df.dropna(subset=['homeworld', 'mass', 'height', 'species'])\n",
    "results_clus = (smf.ols('mass ~ height', data=xf)\n",
    "                   .fit(cov_type='cluster', cov_kwds={'groups': xf['homeworld']}))\n",
    "print(results_clus.summary())"
   ]
  },
  {
   "source": [
    "We can add two-way clustering of standard errors using the following:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf = df.dropna(subset=['homeworld', 'mass', 'height', 'species'])\n",
    "two_way_clusters = np.array(xf[['homeworld', 'species']], dtype=str)\n",
    "results_clus = (smf.ols('mass ~ height', data=xf)\n",
    "                   .fit(cov_type='cluster',\n",
    "                        cov_kwds={'groups': two_way_clusters}))\n",
    "print(results_clus.summary())"
   ]
  },
  {
   "source": [
    "As you would generally expect, the addition of clustering has increased the standard errors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Fixed effects and categorical variables\n",
    "\n",
    "Fixed effects are a way of allowing the intercept of a regression model to vary freely across individuals or groups. It is, for example, used to control for any individual-specific attributes that do not vary across time in panel data.\n",
    "\n",
    "Let's use the 'mtcars' dataset to demonstrate this. We'll read it in and set the datatypes of some of the columns at the same time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (pd.read_csv('https://raw.githubusercontent.com/LOST-STATS/lost-stats.github.io/source/Data/mtcars.csv',\n",
    "                  dtype={'model': str, 'mpg':float, 'hp': float, 'disp': float, 'cyl': \"category\"}))\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "Now we have our data in we want to regress mpg (miles per gallon) on hp (horsepower) with fixed effects for cyl (cylinders). Now we *could* just pop in a formula like this `'mpg ~ hp + cyl'` because we took the trouble to declare that `cyl` was of datatype category when reading it in from the csv file. This means that statsmodels will treat it as a category and use it as a fixed effect by default.\n",
    "\n",
    "But when I read that formula I get nervous that `cyl` might not have been processed correctly (ie it could have been read in as a float, which is what it looks like) and it might just be treated as a float (aka a continuous variable) in the regression. Which is not what we want at all. So, to be safe, and make our intentions explicit (even when the data is of type 'category'), it's best to use the syntax `C(cyl)` to ask for a fixed effect.\n",
    "\n",
    "Here's a regression which does that:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fe = (smf.ols('mpg ~ hp + C(cyl)', data=df)\n",
    "                 .fit())\n",
    "print(results_fe.summary())"
   ]
  },
  {
   "source": [
    "We can see here that two of the three possible values of `cyl`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cyl'].unique()"
   ]
  },
  {
   "source": [
    "have been added as fixed effects regressors. The way that `+C(cyl)` has been added makes it so that the coefficients given are relative to the coefficient for the intercept. We can turn the intercept off to get a coefficient per unique `cyl` value:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smf.ols('mpg ~ hp + C(cyl) -1', data=df)\n",
    "         .fit()\n",
    "         .summary()\n",
    "         .tables[1])"
   ]
  },
  {
   "source": [
    "## Transformations of regressors\n",
    "\n",
    "This chapter is showcasing *linear* regression. What that means is that the model is linear in the regressors: but it doesn't mean that those regressors can't be some kind of (potentially non-linear) transform of the original features $x_i$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Logs and arcsinh\n",
    "\n",
    "You have two options for adding in logs: do them before, or do them in the formula. Doing them before just makes use of standard dataframe operations to declare a new column:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lnhp'] = np.log(df['hp'])\n",
    "print(smf.ols('mpg ~ lnhp', data=df)\n",
    "         .fit()\n",
    "         .summary()\n",
    "         .tables[1])"
   ]
  },
  {
   "source": [
    "Alternatively, you can specify the log directly in the formula:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smf.ols('mpg ~ np.log(hp)', data=df)\n",
    "         .fit()\n",
    "         .summary()\n",
    "         .tables[1])"
   ]
  },
  {
   "source": [
    "Clearly, the first method will work for `arcsinh(x)` and `log(x+1)`, but you can also pass both of these into the formula directly too. (For more on the pros and cons of arcsinh, see {cite}`bellemare2020elasticities`.) Here it is with arcsinh:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smf.ols('mpg ~ np.arcsinh(hp)', data=df)\n",
    "         .fit()\n",
    "         .summary()\n",
    "         .tables[1])"
   ]
  },
  {
   "source": [
    "### Interaction terms and powers\n",
    "\n",
    "This chapter is showcasing *linear* regression. What that means is that the model is linear in the regressors: but it doesn't mean that those regressors can't be some kind of non-linear transform of the original features $x_i$. Two of the most common transformations that you might want to use are *interaction terms* and *polynomial terms*. An example of an interaction term would be\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 \\cdot x_2\n",
    "$$\n",
    "\n",
    "while an example of a polynomial term would be\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1^2\n",
    "$$\n",
    "\n",
    "i.e. the last term enters only after it is multiplied by itself.\n",
    "\n",
    "One note of warning: the interpretation of the effect of a variable is no longer as simple as was set out at the start of this chapter. To work out *what* the new interpretation is, the procedure is the same though: just take the derivative. In the case of the interaction model above, the effect of a unit change in $x_1$ on $y$ is now going to be a function of $x_2$. In the case of the polynomial model above, the effect of a unit change in $x_1$ on $y$ will be $2\\beta_1 \\cdot x_1$.\n",
    "\n",
    "Alright, with all of that preamble out of the way, let's see how we actual do some of this! Let's try including a linear and squared term in the regression of `mpg` on `hp` making use of the numpy power function:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_poly = smf.ols('mpg ~ hp + np.power(hp, 2)', data=df)\n",
    "print(res_poly.fit().summary().tables[1])"
   ]
  },
  {
   "source": [
    "Now let's include the original term in hp, a term in disp, and the interaction between them, which is represented by hp:disp in the table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_inter = smf.ols('mpg ~ hp * disp', data=df)\n",
    "print(res_inter.fit().summary().tables[1])"
   ]
  },
  {
   "source": [
    "In the unusual case that you want *only* the interaction term, you write it as it appears in the table above:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smf.ols('mpg ~ hp : disp', data=df).fit().summary().tables[1])"
   ]
  },
  {
   "source": [
    "## The formula API explained\n",
    "\n",
    "As you will have seen `~` separates the left- and right-hand sides of the regression. `+` computes a set union, which will also be familiar from the examples above (ie it inludes two terms as long as they are distinct). `-` computes a set difference; it adds the set of terms to the left of it while removing any that appear on the right of it. As we've seen, `a*b` is a short-hand for `a + b + a:b`, with the last term representing the interaction. `/` is short hand for `a + a:b`, which is useful if, for example `b` is nested within `a`, so it doesn't make sense to control for `b` on its own. Actually, the `:` character can interact multiple terms so that `(a + b):(d + c)` is the same as `a:c + a:d + b:c + b:d`. `C(a)` tells statsmodels to treat `a` as a categorical variable that will be included as a fixed effect. Finally, as we saw above with powers, you can also pass in vectorised functions, such as `np.log` and `np.power`, directly into the formulae.\n",
    "\n",
    "One gotcha with the formula API is ensuring that you have sensible variable names in your dataframe, i.e. ones that do *not* include whitespace or, to take a really pathological example, have the name 'a + b' for one of the columns that you want to regress on. You can dodge this kind of problem by passing in the variable name as, for example, `Q(\"a + b\")` to be clear that the *column name* is anything within the `Q(\"...\")`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Multiple regression models\n",
    "\n",
    "As is so often the case, you're likely to want to run more than one model at once."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```python\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import statsmodels.api as sm\n",
    "from stargazer.stargazer import Stargazer\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "df = pd.DataFrame(diabetes.data)\n",
    "df.columns = ['Age', 'Sex', 'BMI', 'ABP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']\n",
    "df['target'] = diabetes.target\n",
    "\n",
    "est = sm.OLS(endog=df['target'], exog=sm.add_constant(df[df.columns[0:4]])).fit()\n",
    "est2 = sm.OLS(endog=df['target'], exog=sm.add_constant(df[df.columns[0:6]])).fit()\n",
    "\n",
    "\n",
    "stargazer = Stargazer([est, est2])\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Specifying regressions without formulae, using the array API\n",
    "\n",
    "As noted, there are two ways to run regressions in [**statsmodels**](https://www.statsmodels.org/stable/index.html); passing the data directly as objects, and using formulae. We've seen the formula API, now let's see how to specify regressions using arrays with the format `sm.OLS(y, X)`.\n",
    "\n",
    "We will first need to take the data out of the **pandas** dataframe and put it into a couple of arrays. When we're not using the formula API, the default is to treat the array X as the design matrix for the regression-so, if it doesn't have a column of constants in, there will be no intercept in the regression. Therefore, we need to add a constant vector to the matrix `X` if we *do* want an intercept. Use `sm.add_constant(X)` for this."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(xf['height'])\n",
    "y = np.array(xf['mass'])\n",
    "X = sm.add_constant(X)\n",
    "results = sm.OLS(y, X).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "source": [
    "\n",
    "This approach seems a lot less convenient, not to mention less clear, so you may be wondering when it is useful. It's useful when you want to do many regressions in a systematic way or when you don't know what the columns of a dataset will be called ahead of time. It can actually be a little bit simpler to specify for more complex regressions too."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Fixed effects in the array API\n",
    "\n",
    "If you're using the formula API, it's easy to turn a regressor `x` into a fixed effect by putting `C(x)` into the model formula, as you'll see in the next section.\n",
    "\n",
    "For the array API, things are not that simple and you need to use dummy variables. Let's say we have some data like this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import Generator, PCG64\n",
    "\n",
    "# Set seed for random numbers\n",
    "seed_for_prng = 78557\n",
    "prng = Generator(PCG64(seed_for_prng))\n",
    "\n",
    "no_obs = 200\n",
    "X = pd.DataFrame(prng.normal(size=no_obs))\n",
    "X[1] = prng.choice(['a', 'b'], size=no_obs)\n",
    "# Get this a numpy array\n",
    "X = X.values\n",
    "# Create the y data, adding in a bit of noise\n",
    "y = X[:, 0]*2 + 0.5 + prng.normal(scale=0.1, size=no_obs)\n",
    "y = [el_y + 1.5 if el_x == 'a' else el_y + 3.4 for el_y, el_x in zip(y, X[:, 1])]\n",
    "X[:5, :]"
   ]
  },
  {
   "source": [
    "The first feature (column) is of numbers and it's clear how we include it. The second, however, is a grouping that we'd like to include as a fixed effect. But if we just throw this matrix into `sm.OLS(y, X)`, we're going to get trouble because **statsmodels** isn't sure what to do with a vector of strings. So, instead, we need to create some dummy variables out of our second column of data\n",
    "\n",
    "Astonishingly, there are several popular ways to create dummy variables in Python: **scikit-learn**'s `OneHotEncoder` and **pandas**' `get_dummies` being my favourites. Let's use the latter here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(X[:, 1])"
   ]
  },
  {
   "source": [
    "We just need to pop this into our matrix $X$:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack([X[:, 0], pd.get_dummies(X[:, 1])])\n",
    "X = np.array(X, dtype=float)\n",
    "X[:5, :]"
   ]
  },
  {
   "source": [
    "Okay, so now we're ready to do our regression:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "print(sm.OLS(y, X).fit().summary())"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Perhaps you can see why I generally prefer the formula API..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Review\n",
    "\n",
    "In this very short introduction to regression with code, you should have learned how to:\n",
    "\n",
    "- [x] ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "codeforecon",
   "language": "python",
   "name": "codeforecon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}