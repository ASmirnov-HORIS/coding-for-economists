{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "This chapter covers text analysis, also known as natural language processing. We'll cover tokenisation of text, removing stop words, counting words, performing other statistics on words, and analysing the parts of speech.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "When doing NLP, it's worth thinking carefully about the unit of analysis: is it a corpus, a text, a line, a paragraph, a sentence, a word, or even a character? It could also be two of these simultaneously, and working with document x token matrices is one very common way of doing NLP. Although we'll be mixing between a few of these in this chapter, thinking about what the block of text data you're working with will really help you keep track of what operations are being deployed and how they might interact.\n",
    "\n",
    "In this chapter, we'll use a single example and using NLP on it in a few different ways. First we need to read in the text data we'll be using, part of Adam Smith's *The Wealth of Nations* and do some light cleaning of it though.\n",
    "\n",
    "We'll read in our text so that each new line appears on a different row of a **pandas** dataframe. We'll also import the packages we'll need; remember, if you need these on your computer you may need to run `pip install packagename` on your own computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/aeturrell/coding-for-economists/raw/main/data/smith_won.txt',\n",
    "    delimiter = \"\\n\",\n",
    "    names=[\"text\"])\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "We need to do a bit of light text cleaning before we get on to the more in-depth natural language processing. We'll make use of vectorised string operations as seen in the [Introduction to Text](text-intro) chapter. First, we want to put everything in lower case:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "Next, we'll remove the punctuation from the text. You may not always wish to do this but it's a good default."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = string.punctuation.maketrans({x: \"\" for x in string.punctuation})\n",
    "df[\"text\"] = df[\"text\"].str.translate(translator)\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "Okay, we now have rows and rows of lower case words without punctuation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Tokenisation\n",
    "\n",
    "We're going to now see an example of tokenisation: the process of taking blocks of text and breaking them down into tokens, most commonly a word but potentially all one and two word pairs. Note that you might sometimes see all two word pairs referred to as 2-grams, with an n-gram being all n-word pairs. There are many ways to tokenise text; we'll look at two of the most common: using regular expressions and using pre-configured NLP packages.\n",
    "\n",
    "### Tokenisation with regular expressions\n",
    "\n",
    "Because regular expressions excel at finding patterns in text, they can also be used to decide where to split text up into tokens. For a very simple example, let's take the first line of our text example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "word_pattern = r'\\w+'\n",
    "tokens = re.findall(word_pattern, df.iloc[0, 0])\n",
    "tokens"
   ]
  },
  {
   "source": [
    "This produced a split of a single line into one word tokens that are represented by a list of strings. We could have also asked for other variations, eg sentences, by asking to split at every '.'. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Tokenisation using NLP tools\n",
    "\n",
    "Many of the NLP packages available in Python come with built-in tokenisation tools. Two of the most loved NLP packages are [**nltk**](https://www.nltk.org/) and [**spaCy**](https://spacy.io/). We'll use nltk for tokenisation.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(df.iloc[0, 0])"
   ]
  },
  {
   "source": [
    "We have the same results as before. Now let's scale this up to our whole corpus while retaining the lines of text, giving us a structure of the form (lines x tokens):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df[\"text\"].apply(lambda x: word_tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "**nltk** also has a `sent_tokenize` function that tokenises sentences, although as it makes use of punctuation you must take care with what pre-cleaning of text you undertake.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Stop Words and Their Removal\n",
    "\n",
    "bbb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Counting Text\n",
    "\n",
    "There are several ways of performing basic counting statistics on text. We saw one in the previous chapter, `str.count()`, but that only applies to one word at a time. Often, we're interested in the relative counts of words in a corpus. In this section, we'll look at two powerful ways of computing this: using the `Counter` function and via term frequenc-inverse document frequency.\n",
    "\n",
    "First, `Counter`, which is a built-in Python library that does pretty much what you'd expect. Here's a simple example:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "fruit_list = [\"apple\", \"apple\", \"orange\", \"satsuma\", \"banana\", \"orange\", \"mango\", \"satsuma\", \"orange\"]\n",
    "freq = Counter(fruit_list)\n",
    "freq"
   ]
  },
  {
   "source": [
    "Counter returns a `collections.Counter` object where the numbers of each type in a given input list are summed. The resulting dictionnary of unique counts can be extracted using `dict(freq)`, and `Counter` has some other useful functions too including `most_common()` which, given a number `n`, returns `n` tuples of the form `(thing, count)`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.most_common(10)"
   ]
  },
  {
   "source": [
    "Say we wanted to apply this not just to every line in our corpus separately, but to our whole corpus in one go; how would we do it? `Counter` will happily accept a list but our dataframe token column is currently a vector of lists. So we must first transform the token column to a single list of all tokens and then apply `Counter`. To achieve the former and flatten a list of lists, we'll use `itertools` chain function which makes an iterator that returns elements from the first iterable until it is exhausted, then proceeds to the next iterable, until all of the iterables in all inputs are exhausted. For example, given `[a, b, c]` and `[d, e, f]` as arguments, this function would return `[a, b, c, d, e, f]`. Because this function accepts an arbitrary number of iterable arguments, we use the splat operator to tell it to expect lots of different arguments. The second step using `Counter` is far more straightforward!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "merged_list = list(itertools.chain(*df[\"tokens\"].to_list()))\n",
    "freq = Counter(merged_list)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "source": [
    "### TF-IDF\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeforecon",
   "language": "python",
   "name": "codeforecon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}