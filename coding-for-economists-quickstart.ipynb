{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding for Economists Quickstart\n",
    "\n",
    "There's a trade-off in spending time wading through a book, potentially improving your human capital for the future, when you could be doing something that gives you more immediate utility. So I've created this quickstart tutorial to give you a taste of coding by covering a mini-project from end-to-end. It should take you no more than an hour to run through, and **you can follow it by loading this page interactively in Google Colab by clicking [here]** without installing anything.\n",
    "\n",
    "We'll use a range of techniques that you'll see explained in more detail in the rest of the book, including:\n",
    "\n",
    "- some basic coding;\n",
    "- an example of how to [read in, explore, and clean data](#reading-in-and-clean-data);\n",
    "- an example of performing [analysis](#analysis) on data;\n",
    "- and an example of [presenting](#presenting-results) the results of the analysis."
   ]
  },
  {
   "source": [
    "## Coding\n",
    "\n",
    "A typical segment of computer code might look like this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 10\n",
    "print(variable)"
   ]
  },
  {
   "source": [
    "Here we created an object named `variable`, assigned a value to it (10), and then printed it to screen. An object is just a container for something--it could be a number, a phrase, a function (that takes inputs and creates outputs), a list of other objects. Instead of doing operations on a number directly, `print(10+5)`, objects allow us to perform operations on containers that could have any number in:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "α = 5\n",
    "variable = variable + α\n",
    "print(variable)"
   ]
  },
  {
   "source": [
    "The strength of this is that we can now perform much more impressive operations with a few lines of code. Say we wanted to add 5 to a list of numbers:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_numbers = [10, 20, 30, 40]\n",
    "new_list_of_numbers = [x + 5 for x in list_of_numbers]\n",
    "print(new_list_of_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In high-level open source languages like Python and R, *everything is an object, and every object has a type*. You've already seen two types: integers, like `10` and lists, like `list_of_numbers`. You can always check a type like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "welcome_msg = 'Hello World!'\n",
    "type(welcome_msg)"
   ]
  },
  {
   "source": [
    "Almost all programming languages come with the built-in types integers, floats (real numbers), strings (as above). In Python, there are also lists, dictionaries, sets, tuples, iterators, characters, functions, and so on. The extra packages that you can install to extend the functionality of the base language can add new types, and you can define your own types too. Types work sensibly together by default, for instance you can combine an integer and a float via addition, subtraction, multiplication, etc., and get a sensible answer, and adding two strings together concatenates them.\n",
    "\n",
    "### Functions\n",
    "\n",
    "It's best practice to never repeat yourself in code, the so-called DRY (do not repeat yourself) principle. Functions are the workhorse of not repeating yourself because they can be re-used.\n",
    "\n",
    "In the example below, we'll define a function that adds a number to every element in a list. The text at the top of the function is called a docstring. It gives information on what the function does.\n",
    "\n",
    "\n",
    "By default, the number it adds is 5. But we'll also define the number to add as a *keyword argument*, which means that we can override the default by supplying whatever number we like. The example below also makes use of `range(n)`, which creates a range of numbers from 0 to n-1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_number_to_list(input_list, number=5):\n",
    "    \"\"\"Function that adds a number to each element of a given list.\n",
    "    Default behaviour is to add 5 to the list.\n",
    "    \"\"\"\n",
    "    return [x + number for x in input_list]\n",
    "\n",
    "\n",
    "list_of_numbers = list(range(10))\n",
    "\n",
    "# Use default\n",
    "print(add_number_to_list(list_of_numbers))\n",
    "# Override default\n",
    "print(add_number_to_list(list_of_numbers, number=10))"
   ]
  },
  {
   "source": [
    "The beauty of this approach is that the code can be re-used in different situations. This is a very simple example--in reality, there are more concise ways to do this--but it conveys the idea of re-using code rather than repeating it.\n",
    "\n",
    "Note that the body of the function was *indented*. To mark the difference between the body of a function, for loop, or conditional clause, four spaces are used to indent each level. \n",
    "\n",
    "If you're ever unsure what a function does, just call `help(functionname)` on it:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(add_number_to_list)"
   ]
  },
  {
   "source": [
    "### Installing and using packages\n",
    "\n",
    "As well as not repeating yourself by using functions, it's a good idea to make use of other people's hard work whenever you can. That means importing their code and using it. There's an amazing open source software community producing code that makes a host of operations far easier.\n",
    "\n",
    "To install extra packages, normally you'd open up a command line (also known as the terminal) and write `pip install packagename` to install a package into whatever coding environment you're currently using. If you're using Google Colab to go through this quickstart interactively, just uncomment the first line of the example below by removing the `#` character and the space following it so that the first line begins with an exclamation mark and the pip install command.\n",
    "\n",
    "Below is a simple example of using an installed package, in this case a progress bar called `tdqm`. `sleep`, from the **time** package, is installed by default."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "text = \"\\n Text: \"\n",
    "for bit_of_text in tqdm([\"This\", \"is\", \"a\", \"string\"]):\n",
    "    sleep(0.25)\n",
    "    text = text + ' ' + bit_of_text\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "source": [
    "### Summary\n",
    "\n",
    "There's a lot to think about when it comes to writing (good) code but, as we haven't got long, here are just a few key points:\n",
    "- beautiful, explicit, readable code is much better than complex code--the person who will read your code the most is future you! For example, use meaningful and informative variable names and comments.\n",
    "- Do not repeat yourself! If you find that you are writing the same thing over and over, use a function.\n",
    "- No-one remembers everything in a programming language. Use Stack Overflow (a forum), [cheat sheets](https://gto76.github.io/python-cheatsheet/), and the documentation websites of packages liberally. \n",
    "- Make use of other people's packages to avoid re-inventing the wheel, especially if they are widely used in the community.\n",
    "- We didn't cover it here, but eventually you'll want to use version control to track and save the changes you make in your code. This stops you from having files named 'final_definitelyfinal_v10.py' and instead gives you a full history of every change you've made for every single file.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Read in, explore, and clean data\n",
    "\n",
    "### Reading in data\n",
    "\n",
    "For any and all empirical work, loading up the data you're using is going to be one of the first things you need to do. Fortunately, there are powerful open source solutions to load data in almost any format you can think of. And, in my experience, almost every data cleaning operation is different so bear in mind that in this example you'll just see *one* way that data cleaning might occur.\n",
    "\n",
    "We're going to use the ubiquitous data package [**pandas**](https://pandas.pydata.org/), which has ways to load data from formats including:\n",
    "- csv\n",
    "- Excel\n",
    "- txt\n",
    "- Stata\n",
    "- parquet (a fast, big data format)\n",
    "- json\n",
    "- the clipboard (yes, as in `Ctrl + C`!)\n",
    "- SAS\n",
    "- SPSS\n",
    "- SQL\n",
    "- Feather\n",
    "- HDF5\n",
    "\n",
    "The standard way to call this library in is via `import pandas as pd`. You can give whatever name you like to packages you import (for example, you could `import pandas as supercalifragilisticexpialidocious`), but there are a few conventions around for the very popular packages like **pandas** and shorter import names are going to save you time and effort.\n",
    "\n",
    "It's always easier to read in neatly formatted data but so often we find that real-world data is messy and needs a bit of work to get into shape. To demonstrate how this works in practice, we're going to work with a messy dataset and show how to read it in. You'll have probably heard that 80% of data science is cleaning the data--well it's true for empirical economics too!\n",
    "\n",
    "#### The problem\n",
    "\n",
    "The dataset we'll use is the Ames, Iowa house price data. The objective is to explain the house prices using regression. These data come from a Kaggle competition.\n",
    "\n",
    "Bring in housing deflator to adjust prices.\n",
    "\n",
    "The data come in a csv file, so we'll be using pandas' `read_csv` function. Then we'll take a first look at the first few rows of the data using the `head` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# (Change to link)\n",
    "df = pd.read_csv(os.path.join(\"data\", \"ames_iowa_house_prices.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "### Exploring data\n",
    "\n",
    "\n",
    "What do we know about these data? We can use the `info` function to get a very high-level overview of the dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "source": [
    "This gives us some basic information on quantitative columns, such as their mean and standard deviation. We can also see that there are many (60) columns and 39,644 rows.\n",
    "\n",
    "Really, we want a bit more information than this. Happily, there are more powerful tools we can bring to bear for exploratory data analysis; we'll use the [**pandas profiling**](https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/) package. (Again, you may need to install this first by uncommenting the first line)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas-profiling\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df, title=\"Profiling Report\", minimal=True)\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "source": [
    "This is a full on report about everything in our dataset! We can see, for instance, that we have 32 numerical variables, 1 boolean (1 or 0; true or false), and 48 categorical variables. \n",
    "\n",
    "The warnings page has plenty of useful info on data quality. `Alley` is missing most of its values, for example, `PoolArea` has a highly skewed distribution, and `EnclosedPorch` is mostly filled with zeros. We can also see that, usefully, the `Id` column is unique.\n",
    "\n",
    "Digging down into the detailed reports on each variable and toggling details on, we can see that, for example, `MSZoning` is a categorical variable with highly unbalanced classes.\n",
    "\n",
    "The absolute first and most useful thing you can do with a new dataset is to get to know it, warts and all. Here, we used the most basic pandas profile report (`minimal=True`), but you can opt for ones that include more analysis, although be wary with larger datasets as some extras do not scale well.\n",
    "\n",
    "Let's take a closer look at the variable we're trying to explain, which is `SalePrice`. For quick exploration, we'll use two of the most popular plotting libraries in Python, the imperative (build what you want) library **matplotlib**, which has very good integration with **pandas**, and the declarative (say what you want) library **seaborn**. We'll take a look at `SalePrice` and see if it's roughly log-normal ready for our regression later. We'll also modify the **matplotlib** default plotting style to be a bit more easy on the eye! "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "plt.style.use('seaborn-notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['SalePrice'], bins=100, kde=False, fit=st.lognorm);"
   ]
  },
  {
   "source": [
    "Let's create a logged version of this."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_sale_price'] = np.log(df['SalePrice'])"
   ]
  },
  {
   "source": [
    "Although there are *already* lots of variables in this dataset, we must recognise that it comes from a range of times. In fact, we should look at the time range and sale price to see if there's anything to cause concern there, for instance time trends.\n",
    "\n",
    "First, let's create a proper datetime variable from the given `YrSold` column:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['YrSold'], format='%Y')\n",
    "df['date'].head()"
   ]
  },
  {
   "source": [
    "These are houses that were sold within the given year, but the default date setting has shifted the datetimes to the start of the year. Let's sort that out by shifting the datetimes to the end of the year, to prevent any information problems later in the analysis:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['date'] + pd.offsets.YearEnd()\n",
    "df['date'].head()"
   ]
  },
  {
   "source": [
    "Now let's look at the time trend in sales price:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df, x=\"date\", y=\"SalePrice\");"
   ]
  },
  {
   "source": [
    "It looks like prices fell over the period, which encompasses the Great Financial Crisis. We've no real reason to believe that falling prices were anything to do with local factors to do with buildings or garages in Ames, Iowa so this suggest we might need to bring some broader factors into play in order to explain house prices. The first is to adjust for inflation so that we are dealing with sold prices in *real terms*. Essentially, we want to remove the general effect of rising prices due to inflation.\n",
    "\n",
    "The wonderful data website [FRED]() has a time series which is a house price index that we can use as a deflator, namely the 'All-Transactions House Price Index for the United States', codename USSTHPI. But we need to get that data into the rest of our analysis. Fortunately, there's a package called **pandas-datareader** that exists to connect your analysis with online databases that we can use to pull down the relevant time series:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as pdr\n",
    "start = df['date'].min()\n",
    "end = df['date'].max()\n",
    "hpi_deflator = pdr.get_data_fred('USSTHPI', start, end)\n",
    "hpi_deflator.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpi_deflator.plot();"
   ]
  },
  {
   "source": [
    "Okay, now we need to fold this into our dataset with a merge. Let's use the date column to do this. We can see that the HPI is at a higher frequency than our dataset, so our first task is to down-sample it:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hpi = (hpi_deflator\n",
    "          .groupby(pd.Grouper(freq='A'))\n",
    "          .mean()\n",
    "          .reset_index()\n",
    "          .rename(columns={'DATE': 'date'}))\n",
    "ds_hpi"
   ]
  },
  {
   "source": [
    "Let's now merge these two together while keeping only those entries in `df` that are also in `ds_pi`. Effectively, we are just tacking on the HPI to the existing dataframe. We'll show this by printing the first 5 rows and last four columns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, ds_hpi, on='date', how='left')\n",
    "df.iloc[:5, -4:]"
   ]
  },
  {
   "source": [
    "\n",
    "\n",
    "### Data cleaning\n",
    "\n",
    "As data go, these aren't the messiest ever, but there's still (more) work to do. Ideally we want to work with data in a tidy format, which means one variable per column and one observation per row. To make life easy for ourselves, we also want sensible and informative variable names.\n",
    "\n",
    "#### Variable names\n",
    "\n",
    "Let's take a closer look at some of the variable names:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "source": [
    "Here's a first problem: there is whitespace before some column names. That might make it easy to make an error later on, for example if we look at a column we might type `df['column_name']` instead of `df[' column_name']`. So, as a first step in cleaning this dataset, let's eliminate any leading or trailing whitespace."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns=dict(zip(df.columns, [x.lstrip().rstrip() for x in df.columns])))"
   ]
  },
  {
   "source": [
    "There's a lot to unpack above. The rename function accepts a dictionary, a Python data type, which maps one set of variables into another. An example would be `{' columnname': 'columname'}`, with the mapping going from before the `:` to after. `zip` is a command that pairs every element of a first list with every element of a second list. Finally, the second list that is passed to `zip` consists of the original column names but with leading (`lstrip`) and trailing (`rstrip`) whitespace removed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "source": [
    "### Variable values\n",
    "\n",
    "We should take special care with the values of the variables we're interested in: shares, and num\\_imgs and num\\_videos. Looking at those three in the profiling section, we can see that shares is heavily skewed, with most of the mass around (but greater than zero) and a very small number of larger values. This suggests taking the log of this variable. Secondly, we see the num\\_videos parameter has a 95th percentile of 6, a very small number. The distribution is very skewed, with most values being zero. This suggests whether an article contains a video or not might be a better measure of 'videos' than the number of them. Finally, num\\_imgs has quite a skewed distribution too, but it's not quite as extreme or centred on zero--so we'll leave this variable as it is.\n",
    "\n",
    "Let's put those changes through:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['log_shares'] = np.log(df['shares'])\n",
    "df['video'] = df['num_videos']>0\n",
    "df['video'] = df['video'].astype('category')"
   ]
  },
  {
   "source": [
    "#### Consolidating related categorical variables\n",
    "\n",
    "We can see from the column names that some variables are independent of each other, some are variations (eg max and min), and that some are more closely related and some are even mutually exclusive. We'd like to do some data cleaning to put columns that really refer to the same variable into a single column; remember, we would like to work with tidy data, which has one observation per row, and one variable per column. A weekday cannot both be Tuesday and Wednesday, so these two variables are just one variable. Let's double check this by looking at a random sample of them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[x for x in df.columns if 'week' in x]].sample(10)"
   ]
  },
  {
   "source": [
    "We can see here that our data aren't *quite* mutually exclusive because we have a 'weekday\\_is\\_saturday', a 'weekday\\_is\\_sunday', *and* a 'is_weekend'. Let's drop the 'is\\_weekend' column to ensure that we have genuinely mutually exclusive categories."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('is_weekend', axis=1)"
   ]
  },
  {
   "source": [
    "Now we have mutually exclusive categories, we'll convert all of the weekday\\_is columns into a single new variable called weekday. Our strategy will be to only select those columns with 'week' in them (`df[[x for x in df.columns if 'week' in x]]`), then to find whichever entry is equal to 1 (`eq(1)`), and finally to grab the column name of that entry (`idxmax(axis=1)`). Note that **pandas** follows the convention of `axis=0` for rows and `axis=1` for columns, just as $M_{i,j}$ denotes the ith row and jth column of a matrix $M$ in mathematics."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[x for x in df.columns if 'week' in x]].eq(1).idxmax(axis=1)"
   ]
  },
  {
   "source": [
    "Great, this is exactly what we want. Now all that remains is to put this new column into the dataframe and remove the previous weekday columns. We'll do that and then look at the first column and new last column to check it worked."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wkday'] = df[[x for x in df.columns if 'week' in x]].eq(1).idxmax(axis=1)\n",
    "df = df.drop([x for x in df.columns if 'week' in x], axis=1)\n",
    "df.iloc[:, [0, -1]]"
   ]
  },
  {
   "source": [
    "Weekday isn't the only column that needs adjusting. There's another group of columns that fit the same pattern of being mutually exclusive: the ones beginning data\\_channel\\_is. We want to do exactly the same thing to these. First, we need to check that they are mutually exclusive and that there's a channel label for each and every row. We can do that by summing up the entries for all rows and checking that it equals one. For any that *don't* sum to one, we'll find out what values there are using the `value_counts` function.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chan_cols = [x for x in df.columns if 'data_channel_is' in x]\n",
    "df.loc[df[data_chan_cols].sum(axis=1) != 1, data_chan_cols].sum(axis=1).value_counts()"
   ]
  },
  {
   "source": [
    "So there are 6134 rows with zero, implying that there is no data channel specified. To fix this, we'll add a new column, 'data_channel_is_none', and set it to 1 just for these columns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data_channel_is_none'] = 0\n",
    "df.loc[df[data_chan_cols].sum(axis=1) != 1, 'data_channel_is_none'] = 1\n",
    "# Now check that there are no rows that sum to zero.\n",
    "new_data_chan_cols = [x for x in df.columns if 'data_channel_is' in x]\n",
    "df[df[new_data_chan_cols].sum(axis=1) != 1]"
   ]
  },
  {
   "source": [
    "Fantastic, there's now a data channel specified for every observation. Now we just need to transform those columns so that they are a single column, reflecting the fact that they are really a single variable. But remember, do not repeat yourself! Rather than go through the same steps, we should define a general function that can perform these steps in one go:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def many_cols_to_one_variable(df, var_names_stem, new_var_name):\n",
    "    old_col_list = [x for x in df.columns if var_names_stem in x]\n",
    "    df[new_var_name] = df[old_col_list].eq(1).idxmax(axis=1)\n",
    "    df = df.drop(old_col_list, axis=1)\n",
    "    return df"
   ]
  },
  {
   "source": [
    "Okay, we've defined the function. Now let's use it on the group we'd like to merge down to a single variable:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = many_cols_to_one_variable(df, 'data_channel_is', 'data_channel')\n",
    "df.iloc[:, -4:]"
   ]
  },
  {
   "source": [
    "Printing the last four columns of the dataframe using `iloc[:, -4:]`, we can see that the new columns are included. Now we now have a dataset in the format we want, we're ready to move on to performing the analysis.\n",
    "\n",
    "#### Creating new variables\n",
    "\n",
    "Or are we? We've thought about the variables in our data, but is there any important external information that we could include, or variables we could derive? In this case, yes. It could well be that the number of shares is changing in time due to the overall popularity of mashable as a website. Let's say, for the purposes of this tutorial that we don't have the column called 'timedelta', but we do want to account for time somehow. How would we do it?\n",
    "\n",
    "We can see that the url variable contains information about the date and time, although it's in an awkward format. Let's do some simple text analysis to grab a datetime from the url column. Each entry has the format 'http://www.mashable.com/year/month/day/article-name'. That's going to make it easy for us to extract by splitting the string and keeping only the parts we want. Let's try it on an example (the first entry). We can see that the 3rd, 4th, and 5th entries of each string split by '/' will give us the parts of the date we want:\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0, 0])\n",
    "print(df.iloc[0, 0].split('/'))\n",
    "print(df.iloc[0, 0].split('/')[3:6])"
   ]
  },
  {
   "source": [
    "Now we need to i) apply this to all of the urls and ii) turn it into a datetime, a special datatype for dealing with dates and times. Fortunately, **pandas** has us covered for both of these."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['url'].apply(lambda x: '-'.join(x.split('/')[3:6])))\n",
    "df['date']"
   ]
  },
  {
   "source": [
    "Let's use this to see if there are any time trends in the number of shares and plot the results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.set_index(['date'])\n",
    "   .groupby([pd.Grouper(freq='M')])['shares']\n",
    "   .mean()\n",
    "   .plot())"
   ]
  },
  {
   "source": [
    "It looks like there may be some seasonality here that we should account for. But there are only two years' worth of data, so using high frequency fixed effects to control for seasonality isn't going to work. We'll use quarterly dummy variables."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quarter'] = df['date'].apply(lambda x: x.quarter)"
   ]
  },
  {
   "source": [
    "### Summary\n",
    "\n",
    "Reading in, exploring, and cleaning data are usually the hardest parts of (economic) data science. Data can catch you out so easily, and in so many different ways. The example above is very bespoke to the data, and this is typical of all data cleaning and preparation exercises. We didn't even see common operations like merging two different datasets! The best advice I can give is to start experimenting with data cleaning in order to come across some common themes.\n",
    "\n",
    "Remember:\n",
    "- understanding your data is most of the battle--running a model on cleaned data is the easy part\n",
    "- how you read, explore, and clean your data will depend entirely on the question you are trying to answer\n",
    "- \n",
    "\n",
    "## Analysis\n",
    "\n",
    "We will now try to explain the number of shares ('shares' in the dataset) of an article based on characteristics of the article in the dataset. Specifically, we are interested in whether having rich media, such as images and video, helps increases the shares of articles. We can do that by using ordinary least squares (OLS) to regress shares on the variables representing the amount of rich media content. \n",
    "\n",
    "Let's start with the simplest model we can think of, which is just regressing the log(shares) on the fixed effects from the weekday and data channel as well as the number of images, number of videos, and number of links to other articles. We'll use the [**statsmodels**] package and its formula API. This lets us use text to specify a model we want to estimate. Putting 'C(variable_name)' into the formula tells statsmodels to treat that variable as a categorical variable, also known as a fixed effect.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "model_basic = smf.ols('log_shares ~ C(data_channel)  + C(wkday) + num_imgs + C(video) + num_hrefs + C(quarter)', data=df)\n",
    "results_basic = model_basic.fit(cov_type='HC1')\n",
    "print(results_basic.summary())"
   ]
  },
  {
   "source": [
    "So it looks like a unit increase in the number of images is associated with a 0.46% increase in the number of shares.\n",
    "\n",
    "However, there are a LOT of other variables in this dataset that we haven't used. Omitting them could be influencing the parameters we're seeing. So actually the first thing we should be doing is considering whether we need to include these other variables. As many of them could also have an influence on shares, we probably should--but there are just so many!\n",
    "\n",
    "The easiest way to thing about them is to break them down into similar groups of variables. There are some that count tokens (eg individual words), some looking at sentiment and polarity, and some looking at the title of the article. Then there are a few miscellaneous ones left over (such as url, which we can safely not use in the regression). Let's try and group these use Python's list comprehensions.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vars = [x for x in df.columns if 'token' in x]\n",
    "sentiment_vars = [x for x in df.columns if ((('sentiment' in x) or ('polarity' in x)) and ('title' not in x))]\n",
    "keyword_vars = [x for x in df.columns if 'kw' in x]\n",
    "title_vars = [x for x in df.columns if (('title' in x) and (x not in token_vars))]\n",
    "# Let's look at one of these as an example:\n",
    "print(', \\t'.join(title_vars))"
   ]
  },
  {
   "source": [
    "Great, there are now four distinct groups of variables in addition to the ones that were already considered.\n",
    "\n",
    "We *could* just throw everything into a model (the kitchen sink approach) but some of the variables in the data are likely to be very highly correlated, and multi-collinearity will create issues for our regression. Let's first look at whether any of the variables we haven't already discussed are highly correlated. Just taking the correlation of *all* of the variables will create a huge matrix, so we'll also cut it down to pairs that are highly correlated.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = df[token_vars + title_vars + sentiment_vars + keyword_vars].corr()\n",
    "# Grab any pairs that are problematic\n",
    "corr_mat[((corr_mat>0.7) | (corr_mat<-0.7)) & (corr_mat != 1)].dropna(how='all', axis=0).dropna(how='all', axis=1).fillna('')"
   ]
  },
  {
   "source": [
    "It's clear from this there are quite a few pairs of correlated variables within each group of variables, and we should be cautious about lumping them all in together.\n",
    "\n",
    "We have many choices at this point but, without going into too much detail, we could either remove some of the independent variables or combine them. In this case, we think that there could still be useful information in the variables so we'd like to keep them but whittle down their information to fewer variables. This sounds like a job for unsupervised machine learning!\n",
    "\n",
    "### Dimensional reduction\n",
    "\n",
    "We will make use of the UMAP algorithm to take the sets of variables we've identified, which consists of 25 variables in total, and squish them down to just four dimensions (variables), on the basis that we there are probably only really four different bits of information here.\n",
    "\n",
    "We'll also make use of a scaler, an algorithm that puts the different data on the same scale. This helps the UMAP algorithm more effectively perform dimensional reduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Scale the variables:\n",
    "scaled_vars = StandardScaler().fit_transform(df[token_vars + sentiment_vars + keyword_vars + title_vars].values)\n",
    "scaled_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_num_dims = 4\n",
    "reducer = umap.UMAP(n_components=final_num_dims)\n",
    "embedding = reducer.fit_transform(scaled_vars)\n",
    "# Print the dimension of the matrix\n",
    "print(embedding.shape)"
   ]
  },
  {
   "source": [
    "As you can see, we now have a matrix called embedding that has dimensions (no rows)x4 instead of our original (no rows)x25. We need to put this data into the original dataframe:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(final_num_dims):\n",
    "    col_name = 'umap_' + str(i)\n",
    "    df[col_name] = embedding[:, i]\n",
    "# Check the correlation of the new columns\n",
    "df[['umap_'+str(i) for i in range(final_num_dims)]].corr()"
   ]
  },
  {
   "source": [
    "Although there is still *some* correlation between these dimensions, it's much less serious than what we began with--and the 25 variables are now just 4. So we can now fit the model using the UMAP-derived columns as extra variables."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_string = ' + '.join(['umap_'+str(i) for i in range(final_num_dims)])\n",
    "model = smf.ols('log_shares ~ C(data_channel)  + C(wkday) + num_imgs + C(video) + num_hrefs + C(quarter) +' + umap_string, data=df)\n",
    "results = model.fit(cov_type='HC1')\n",
    "print(results.summary())"
   ]
  },
  {
   "source": [
    "This gives us a slightly different value for the impact of the number of videos on the percentage of shares of an article: 0.38%, versus 0.46% from earlier. How can we square these? They did use slightly different specifications. In fact, there were many choices of specification we could have made throughout this process. This garden of forking paths is a problem if we want to have confidence in the relationship that we're interested in; the results should not be fragile to small changes in specification.\n",
    "\n",
    "Fortunately, there are ways to think about this more comprehensively. One trick is to use *specification curve analysis*. This looks at a range of plausible specifications and plots them out. By comparing so many specifications, we get a better idea of whether the preferred specification is a fragile outlier or a robust results.\n",
    "\n",
    "We'll create a specification curve for the association between the number of images and the number of shares using the [**specification_curve**](https://specification-curve.readthedocs.io/en/latest/readme.html) package (disclaimer: I wrote this package!)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from specification_curve import specification_curve as specy\n",
    "sc = specy.SpecificationCurve(df, 'log_shares', 'num_imgs',\n",
    "                              ['umap_0', 'umap_1', 'umap_2', 'umap_3', 'num_hrefs', 'wkday', 'data_channel', 'quarter', 'video'],\n",
    "                              always_include=['video', 'num_hrefs'])\n",
    "sc.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.plot(preferred_spec=['log_shares', 'num_imgs', 'umap_0', 'umap_1', 'umap_2', 'umap_3',\n",
    "                        'num_hrefs', 'wkday', 'data_channel', 'quarter', 'video'])"
   ]
  },
  {
   "source": [
    "Looking at the specification curve, we can see that most estimates are clustered around the 0.35%--0.50% range *if* the number of links and video fixed effect are both included as regressors. These are both similar to the variable we're interested in, so it seems reasonable to always include them. The preferred specification is right at the lower end of the range, but includes all of the controls.\n",
    "\n",
    "### Summary\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Presenting results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('codeforecon': conda)",
   "metadata": {
    "interpreter": {
     "hash": "671f4d32165728098ed6607f79d86bfe6b725b450a30021a55936f1af379a247"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}